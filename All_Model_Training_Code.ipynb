{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2645886,"sourceType":"datasetVersion","datasetId":1608934},{"sourceId":217223,"sourceType":"modelInstanceVersion","modelInstanceId":185219,"modelId":207359},{"sourceId":217963,"sourceType":"modelInstanceVersion","modelInstanceId":185874,"modelId":207995}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# __WOA7015 Alternative Assessment__\nGroup: __Learning Machine__\n\nFinal Model","metadata":{"id":"mFzVqyyy96u9"}},{"cell_type":"markdown","source":"## Import Needed Modules","metadata":{"id":"CuEOx57tnjoo"}},{"cell_type":"code","source":"import torch\nimport time\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, random_split, Dataset\n# from torchvision.models import resnet18, VGG16_Weights, ResNet18_Weights, AlexNet_Weights\nfrom torchvision import models, transforms\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.utils import make_grid\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport json\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    f1_score,\n    precision_score,\n    recall_score,\n    confusion_matrix,\n    classification_report\n)\nimport pandas as pd\nimport logging\nfrom datetime import datetime\nimport math\nfrom typing import Optional, Tuple, List, Dict, Any\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.tensorboard import SummaryWriter\nimport io\nimport seaborn as sns\nimport logging\nfrom io import BytesIO\nfrom PIL import Image\nimport csv\nimport pandas as pd ","metadata":{"id":"3BPS_Cgknjor","trusted":true,"execution":{"iopub.status.busy":"2025-01-07T14:12:50.192170Z","iopub.execute_input":"2025-01-07T14:12:50.192628Z","iopub.status.idle":"2025-01-07T14:13:03.104483Z","shell.execute_reply.started":"2025-01-07T14:12:50.192587Z","shell.execute_reply":"2025-01-07T14:13:03.103794Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!rm -rf /kaggle/working/*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-07T14:13:03.105526Z","iopub.execute_input":"2025-01-07T14:13:03.105970Z","iopub.status.idle":"2025-01-07T14:13:03.244301Z","shell.execute_reply.started":"2025-01-07T14:13:03.105947Z","shell.execute_reply":"2025-01-07T14:13:03.243240Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Set the device for training\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"id":"vtrgujlwnjos","outputId":"aff5381f-f421-44cb-dd23-e29596731a1a","trusted":true,"execution":{"iopub.status.busy":"2025-01-07T14:13:03.246216Z","iopub.execute_input":"2025-01-07T14:13:03.246480Z","iopub.status.idle":"2025-01-07T14:13:03.303968Z","shell.execute_reply.started":"2025-01-07T14:13:03.246459Z","shell.execute_reply":"2025-01-07T14:13:03.303197Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## Get Data","metadata":{"id":"TmwKAIHXnjot"}},{"cell_type":"code","source":"# Baseline: RGB\ntrain_data_dataloader_baseline = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomApply([\n        transforms.RandomRotation(degrees=15),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1))\n    ], p=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nval_data_dataloader_baseline = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Custom: RGB\ntrain_data_dataloader = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(p=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nval_data_dataloader = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Custom: Gray\ntrain_data_dataloader_gry = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(p=0.2),\n\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])\n\nval_data_dataloader_gry = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])\n\n# dataset: https://drive.google.com/drive/folders/1d1ArqNswahfsmP6h-fm3WCvL47dVOpxA?usp=drive_link\n\n# Define the base directory for data\nbase_data_dir = '/kaggle/input/brain-tumor-mri-dataset'\n\n#1. Baseline: RGB\ntrain_dataset_base = ImageFolder(f'{base_data_dir}/Training', transform=train_data_dataloader_baseline)\ntrain_loader_base = DataLoader(train_dataset_base, batch_size=128, shuffle=True)  # batch_size set to 128 due to \"CUDA out of memory\" when using VGG16 model\n\nval_dataset_base = ImageFolder(f'{base_data_dir}/Testing', transform=val_data_dataloader_baseline)\nval_loader_base = DataLoader(val_dataset_base, batch_size=128, shuffle=False)\n\n# 2. Custom: RGB\ntrain_dataset = ImageFolder(f'{base_data_dir}/Training', transform=train_data_dataloader)\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n\nval_dataset = ImageFolder(f'{base_data_dir}/Testing', transform=val_data_dataloader)\nval_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n\n# 3. Custom: Gray\ntrain_dataset_gry = ImageFolder(f'{base_data_dir}/Training', transform=train_data_dataloader_gry)\ntrain_loader_gry = DataLoader(train_dataset_gry, batch_size=128, shuffle=True)\n\nval_dataset_gry = ImageFolder(f'{base_data_dir}/Testing', transform=val_data_dataloader_gry)\nval_loader_gry = DataLoader(val_dataset_gry, batch_size=128, shuffle=False)\n","metadata":{"id":"sfnlwRlpnjou","trusted":true,"execution":{"iopub.status.busy":"2025-01-07T14:13:03.305116Z","iopub.execute_input":"2025-01-07T14:13:03.305435Z","iopub.status.idle":"2025-01-07T14:13:10.101695Z","shell.execute_reply.started":"2025-01-07T14:13:03.305411Z","shell.execute_reply":"2025-01-07T14:13:10.100883Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Custom Model Architecture","metadata":{"id":"M39WRjADnjox"}},{"cell_type":"code","source":"class EfficientMultiHeadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout=0.1):\n        super().__init__()\n        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.scale = self.head_dim ** -0.5\n\n        # Use a single projection for QKV to reduce computation\n        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim)\n        self.out_proj = nn.Linear(embed_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        B, L, C = x.shape\n\n        # Efficient QKV projection\n        qkv = self.qkv_proj(x).chunk(3, dim=-1)\n        q, k, v = map(lambda t: t.view(B, L, self.num_heads, self.head_dim).transpose(1, 2), qkv)\n\n        # Use einsum for more efficient attention computation\n        attn_weights = torch.einsum('bhqd,bhkd->bhqk', q, k) * self.scale\n\n        if mask is not None:\n            attn_weights = attn_weights.masked_fill(mask == 0, float('-inf'))\n\n        attn_probs = F.softmax(attn_weights, dim=-1)\n        attn_probs = self.dropout(attn_probs)\n\n        context = torch.einsum('bhqk,bhkd->bhqd', attn_probs, v)\n        context = context.transpose(1, 2).contiguous().view(B, L, C)\n\n        return self.out_proj(context)\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.05):\n        super().__init__()\n        self.attention = EfficientMultiHeadAttention(embed_dim, num_heads, dropout)\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.norm2 = nn.LayerNorm(embed_dim)\n\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, mlp_dim),\n            nn.GELU(),\n            #nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        x = x + self.attention(x)\n        x = x + self.mlp(x)\n        return x\n\nclass RotaryPositionalEmbedding(nn.Module):\n    def __init__(self, dim, base=10000):\n        super().__init__()\n        inv_freq = 1. / (base ** (torch.arange(0., dim, 2.) / dim))\n        self.register_buffer('inv_freq', inv_freq)\n\n    def forward(self, x, seq_dim=1):\n        t = torch.arange(x.shape[seq_dim], device=x.device).type_as(self.inv_freq)\n        sinusoid_inp = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n        emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n        return emb[None, :, :]\n\nclass HybridFPNTransformer_gry(nn.Module):\n    def __init__(self, num_classes, image_size=224, patch_size=16,\n                 embed_dim=128, num_heads=8, transformer_depth=4):\n        super().__init__()\n        \n        # Stem network remains the same - processes raw images\n        self.stem = nn.Sequential(\n            nn.Conv2d(1, embed_dim*2, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(embed_dim*2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(embed_dim*2, embed_dim, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(embed_dim),\n            nn.ReLU(inplace=True)\n        )\n\n        # Patch embedding remains the same\n        self.patch_embed = nn.Conv2d(\n            embed_dim, embed_dim,\n            kernel_size=patch_size,\n            stride=patch_size\n        )\n\n        # Rotary positional embeddings for spatial awareness\n        self.rotary_emb = RotaryPositionalEmbedding(embed_dim)\n\n        # Transformer blocks remain unchanged\n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads, embed_dim)\n            for _ in range(transformer_depth)\n        ])\n\n        # Simplified classification head - directly from transformer output\n        self.classifier = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n            nn.Linear(embed_dim, 64),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.1),\n            nn.Linear(64, num_classes)\n        )\n\n    def forward(self, x):\n        # Initial feature extraction\n        x = self.stem(x)\n        x_patches = self.patch_embed(x)\n\n        # Reshape for transformer processing\n        B, C, H, W = x_patches.shape\n        x_patches = x_patches.flatten(2).transpose(1, 2)\n\n        # Add positional information\n        rotary_pos_emb = self.rotary_emb(x_patches)\n        x_patches = x_patches + rotary_pos_emb\n\n        # Process through transformer blocks\n        for transformer_block in self.transformer_blocks:\n            x_patches = transformer_block(x_patches)\n\n        # Reshape back to 2D and classify\n        x_transformed = x_patches.transpose(1, 2).view(B, -1, H, W)\n        return self.classifier(x_transformed)\n\n\nclass HybridFPNTransformer(nn.Module):\n    def __init__(self, num_classes, image_size=224, patch_size=16,\n                 embed_dim=128, num_heads=8, transformer_depth=4):\n        super().__init__()\n        \n        # Stem network remains the same - processes raw images\n        self.stem = nn.Sequential(\n            nn.Conv2d(3, embed_dim*2, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(embed_dim*2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(embed_dim*2, embed_dim, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(embed_dim),\n            nn.ReLU(inplace=True)\n        )\n\n        # Patch embedding remains the same\n        self.patch_embed = nn.Conv2d(\n            embed_dim, embed_dim,\n            kernel_size=patch_size,\n            stride=patch_size\n        )\n\n        # Rotary positional embeddings for spatial awareness\n        self.rotary_emb = RotaryPositionalEmbedding(embed_dim)\n\n        # Transformer blocks remain unchanged\n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads, embed_dim)\n            for _ in range(transformer_depth)\n        ])\n\n        # Simplified classification head - directly from transformer output\n        self.classifier = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n            nn.Linear(embed_dim, 64),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.1),\n            nn.Linear(64, num_classes)\n        )\n\n    def forward(self, x):\n        # Initial feature extraction\n        x = self.stem(x)\n        x_patches = self.patch_embed(x)\n\n        # Reshape for transformer processing\n        B, C, H, W = x_patches.shape\n        x_patches = x_patches.flatten(2).transpose(1, 2)\n\n        # Add positional information\n        rotary_pos_emb = self.rotary_emb(x_patches)\n        x_patches = x_patches + rotary_pos_emb\n\n        # Process through transformer blocks\n        for transformer_block in self.transformer_blocks:\n            x_patches = transformer_block(x_patches)\n\n        # Reshape back to 2D and classify\n        x_transformed = x_patches.transpose(1, 2).view(B, -1, H, W)\n        return self.classifier(x_transformed)\n","metadata":{"id":"f_aCuXcvnjox","trusted":true,"execution":{"iopub.status.busy":"2025-01-07T14:13:10.102690Z","iopub.execute_input":"2025-01-07T14:13:10.103003Z","iopub.status.idle":"2025-01-07T14:13:10.121173Z","shell.execute_reply.started":"2025-01-07T14:13:10.102971Z","shell.execute_reply":"2025-01-07T14:13:10.120022Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def setup_logging(\n    log_dir: str,\n    log_level: int = logging.INFO,\n    verbose: bool = False\n) -> logging.Logger:\n    os.makedirs(log_dir, exist_ok=True)\n\n    # Generate more detailed log filename\n    log_filename = os.path.join(\n        log_dir,\n        f'tumor_classification_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n    )\n\n    # Configure logging with more details\n    logging.basicConfig(\n        level=log_level,\n        format='%(asctime)s - %(name)s - %(levelname)s: %(message)s',\n        handlers=[\n            logging.FileHandler(log_filename),\n            logging.StreamHandler()\n        ]\n    )\n\n    logger = logging.getLogger('TumorClassification')\n\n    # Add hardware information logging\n    if verbose:\n        logger.info(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n        logger.info(f\"CUDA Available: {torch.cuda.is_available()}\")\n\n    return logger\n\ndef calculate_metrics(true_labels, pred_labels, num_classes):\n    # Calculate metrics with weighted average for multiclass\n    f1 = f1_score(true_labels, pred_labels, average='weighted')\n    precision = precision_score(true_labels, pred_labels, average='weighted')\n    recall = recall_score(true_labels, pred_labels, average='weighted')\n\n    # Confusion Matrix\n    cm = confusion_matrix(true_labels, pred_labels)\n\n    # Sensitivity (Per Class) derived from confusion matrix\n    sensitivity = []\n    for i in range(num_classes):\n        tp = cm[i, i]  # True positives for class i\n        fn = cm[i, :].sum() - tp  # False negatives for class i\n        sensitivity.append(tp / (tp + fn) if (tp + fn) > 0 else 0)\n\n    return {\n        'f1_score': f1,\n        'precision': precision,\n        'recall': recall,\n        'sensitivity': sensitivity,\n        'confusion_matrix': cm\n    }\ndef save_metrics_to_csv(train_metrics, val_metrics, csv_path):\n    # Ensure the directory exists\n    os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n\n    with open(csv_path, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        # Write the header\n        headers = ['Epoch', 'Train_F1', 'Train_Precision', 'Train_Recall', \n                   'Train_Sensitivity', 'Train_Confusion_Matrix',\n                   'Val_F1', 'Val_Precision', 'Val_Recall', \n                   'Val_Sensitivity', 'Val_Confusion_Matrix']\n        writer.writerow(headers)\n        \n        # Write metrics for each epoch\n        for epoch, (train, val) in enumerate(zip(train_metrics, val_metrics), start=1):\n            writer.writerow([\n                epoch,\n                train['f1_score'],\n                train['precision'],\n                train['recall'],\n                train['sensitivity'],\n                str(train['confusion_matrix'].tolist()),  # Convert to string\n                val['f1_score'],\n                val['precision'],\n                val['recall'],\n                val['sensitivity'],\n                str(val['confusion_matrix'].tolist())  # Convert to string\n            ])\n    print(f\"Metrics saved to {csv_path}\")\n\ndef plot_metrics(train_metrics, val_metrics, save_dir):\n    # Ensure the save directory exists\n    os.makedirs(save_dir, exist_ok=True)\n\n    # Prepare metrics for plotting\n    epochs = range(1, len(train_metrics) + 1)\n    metrics_to_plot = ['f1_score', 'precision', 'recall', 'sensitivity']\n\n    for metric in metrics_to_plot:\n        plt.figure(figsize=(10, 6))\n        train_values = [epoch_metrics[metric] if metric != 'sensitivity' else np.mean(epoch_metrics[metric]) for epoch_metrics in train_metrics]\n        val_values = [epoch_metrics[metric] if metric != 'sensitivity' else np.mean(epoch_metrics[metric]) for epoch_metrics in val_metrics]\n\n        plt.plot(epochs, train_values, label=f'Train {metric}', marker='o')\n        plt.plot(epochs, val_values, label=f'Validation {metric}', marker='o')\n        \n        plt.title(f'{metric.capitalize()} over Epochs')\n        plt.xlabel('Epochs')\n        plt.ylabel(metric.capitalize())\n        plt.legend()\n        plt.grid(True)\n\n        plt.ylim(0, 1)\n\n        # Save the plot as an image\n        save_path = os.path.join(save_dir, f'{metric}.png')\n        plt.savefig(save_path)\n        print(f\"Saved plot for {metric} to {save_path}\")\n        plt.close()\n        \ndef train_model_enhanced(\n    model: nn.Module,\n    train_loader: torch.utils.data.DataLoader,\n    val_loader: torch.utils.data.DataLoader,\n    criterion: nn.Module,\n    optimizer: torch.optim.Optimizer,\n    num_epochs: int = 150,\n    device: Optional[torch.device] = None,\n    num_classes: int = 4,\n    log_dir: str = './logs',\n    checkpoint_dir: str = '/kaggle/working',\n    model_name  = None,\n    early_stopping_patience: int = 10,\n    learning_rate_patience: int = 5\n) -> Tuple[nn.Module, List[Dict], List[Dict]]:\n\n    # Device management\n    device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    checkpoint_dir = os.path.join(checkpoint_dir, model_name)\n    log_dir = os.path.join(log_dir, model_name)\n    os.makedirs(log_dir, exist_ok=True)\n    os.makedirs(checkpoint_dir, exist_ok=True)\n\n    # Logging and TensorBoard\n    logger = setup_logging(log_dir, verbose=True)\n    writer = SummaryWriter(log_dir=log_dir)\n\n    # Learning rate scheduler\n    scheduler = ReduceLROnPlateau(\n        optimizer,\n        mode='max',\n        factor=0.5,\n        patience=learning_rate_patience\n    )\n\n\n\n    # Early stopping variables\n    best_val_f1 = 0.0\n    epochs_no_improve = 0\n\n    train_metrics, val_metrics = [], []\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        train_preds, train_true = [], []\n\n        for batch_idx, (inputs, labels) in enumerate(train_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n\n            # Optional gradient clipping\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n            optimizer.step()\n\n            train_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            train_preds.extend(predicted.cpu().numpy())\n            train_true.extend(labels.cpu().numpy())\n\n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        val_preds, val_true = [], []\n\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n\n                val_loss += loss.item()\n                _, predicted = torch.max(outputs, 1)\n                val_preds.extend(predicted.cpu().numpy())\n                val_true.extend(labels.cpu().numpy())\n\n        # Metrics calculation (similar to original function)\n        train_metrics_epoch = calculate_metrics(\n            np.array(train_true),\n            np.array(train_preds),\n            num_classes\n        )\n        val_metrics_epoch = calculate_metrics(\n            np.array(val_true),\n            np.array(val_preds),\n            num_classes\n        )\n\n        # TensorBoard logging\n        writer.add_scalar('Loss/Train', train_loss/len(train_loader), epoch)\n        writer.add_scalar('Loss/Validation', val_loss/len(val_loader), epoch)\n        writer.add_scalar('F1/Train', train_metrics_epoch['f1_score'], epoch)\n        writer.add_scalar('F1/Validation', val_metrics_epoch['f1_score'], epoch)\n        writer.add_scalar('Precision/Train', train_metrics_epoch['precision'], epoch)\n        writer.add_scalar('Precision/Validation', val_metrics_epoch['precision'], epoch)\n        writer.add_scalar('Recall/Train', train_metrics_epoch['recall'], epoch)\n        writer.add_scalar('Recall/Validation', val_metrics_epoch['recall'], epoch)\n        writer.add_scalar('Sensitivity/Train', np.mean(train_metrics_epoch['sensitivity']), epoch)\n        writer.add_scalar('Sensitivity/Validation', np.mean(val_metrics_epoch['sensitivity']), epoch)\n\n        # Learning rate scheduling\n        scheduler.step(val_metrics_epoch['f1_score'])\n        print(val_metrics_epoch['f1_score'])\n        # Early stopping\n        if val_metrics_epoch['f1_score'] > best_val_f1 and epoch >=10:\n            best_val_f1 = val_metrics_epoch['f1_score']\n            #print(best_val_f1)\n            epochs_no_improve = 0\n\n            # Save best model\n            torch.save({\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'best_f1': best_val_f1,\n                'epoch': epoch\n            }, os.path.join(checkpoint_dir, f'{model_name}_{epoch}_f1_{best_val_f1:.4f}.pth'))\n\n\n        # Log epoch summary\n        logger.info(f\"Epoch {epoch+1}/{num_epochs}\")\n        logger.info(f\"Train F1: {train_metrics_epoch['f1_score']:.4f}\")\n        logger.info(f\"Val F1: {val_metrics_epoch['f1_score']:.4f}\")\n\n        # Early stopping check\n        if epochs_no_improve >= early_stopping_patience:\n            logger.info(f\"Early stopping triggered after {epoch+1} epochs\")\n            break\n\n        train_metrics.append(train_metrics_epoch)\n        val_metrics.append(val_metrics_epoch)\n\n    # Close TensorBoard writer\n    writer.close()\n\n    return model, train_metrics, val_metrics\n\n","metadata":{"id":"hSdh0S5iHbq-","trusted":true,"execution":{"iopub.status.busy":"2025-01-07T14:13:10.122255Z","iopub.execute_input":"2025-01-07T14:13:10.122558Z","iopub.status.idle":"2025-01-07T14:13:10.155295Z","shell.execute_reply.started":"2025-01-07T14:13:10.122533Z","shell.execute_reply":"2025-01-07T14:13:10.154528Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"EPOCHS_NUM = 160\ndef main():\n    # Device configuration\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    # Log directory setup\n    log_dir = './logs/'\n    os.makedirs(log_dir, exist_ok=True)\n\n    # Define model dictionary\n    model_dict = {\n        'VGG16': {\n            'model': models.vgg16(),\n            'weights_path': '/kaggle/input/model/pytorch/default/1/vgg16-397923af.pth'\n        },\n        'ResNet18': {\n            'model': models.resnet18(),\n            'weights_path': '/kaggle/input/model/pytorch/default/1/resnet18-f37072fd.pth'\n        },\n        'AlexNet': {\n            'model': models.alexnet(),\n            'weights_path': '/kaggle/input/model/pytorch/default/1/alexnet-owt-7be5be79.pth'\n        }\n    }\n\n    # # CSV to store training time and parameters\n    time_metrics = []\n\n    # Iterate through base models for experimentation\n    for model_name, model_info in model_dict.items():\n        try:\n            # Load pre-trained weights and adjust the classifier layer\n            base_model = model_info['model']\n            weights_path = model_info['weights_path']\n            base_model.load_state_dict(torch.load(weights_path))\n            \n            if model_name in ['VGG16', 'AlexNet']:\n                base_model.classifier[-1] = nn.Linear(base_model.classifier[-1].in_features, 4)  # Adjust for 4 classes\n            elif model_name == 'ResNet18':\n                base_model.fc = nn.Linear(base_model.fc.in_features, 4)  # Adjust for 4 classes\n            \n            base_model = base_model.to(device)\n\n            # Define loss and optimizer\n            criterion = nn.CrossEntropyLoss()\n            optimizer = optim.Adam(\n                base_model.parameters(),\n                lr=1e-4,\n                weight_decay=1e-5  # L2 regularization\n            )\n\n            # Record start time\n            start_time = time.time()\n\n            # Train the model\n            trained_model, train_metrics, val_metrics = train_model_enhanced(\n                model=base_model,\n                train_loader=train_loader_base,  # Assuming this is defined elsewhere\n                val_loader=val_loader_base,      # Assuming this is defined elsewhere\n                criterion=criterion,\n                optimizer=optimizer,\n                num_epochs=EPOCHS_NUM,\n                model_name=model_name,\n                device=device,\n                num_classes=4\n            )\n\n            # Record end time and calculate training duration\n            end_time = time.time()\n            training_time = end_time - start_time\n\n            # Save metrics to CSV\n            csv_path = os.path.join(log_dir, f'{model_name}_metrics.csv')\n            save_metrics_to_csv(train_metrics, val_metrics, csv_path)\n\n            # Save metric plots\n            plot_path = os.path.join(log_dir, f'{model_name}_plots.png')\n            plot_metrics(train_metrics, val_metrics, plot_path)\n\n            # Log training time and parameters\n            time_metrics.append({\n                'Model': model_name,\n                'Training Time (seconds)': training_time,\n                'Number of Parameters': sum(p.numel() for p in base_model.parameters() if p.requires_grad)\n            })\n\n            # Clear VRAM\n            del base_model, criterion, optimizer, trained_model\n            torch.cuda.empty_cache()\n\n        except Exception as e:\n            print(f\"Error processing {model_name}: {e}\")\n\n    # # # Save training time and parameter metrics to a CSV\n    time_metrics_path = os.path.join(log_dir, 'training_time_metrics.csv')\n    # pd.DataFrame(time_metrics).to_csv(time_metrics_path, index=False)\n\n    # HybridFPNTransformer training\n    hybrid_model = HybridFPNTransformer(num_classes=4).to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(\n        hybrid_model.parameters(),\n        lr=1e-4,\n        weight_decay=1e-5  # L2 regularization\n    )\n\n    start_time = time.time()\n    trained_hybrid_model, hybrid_train_metrics, hybrid_val_metrics = train_model_enhanced(\n        model=hybrid_model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        criterion=criterion,\n        optimizer=optimizer,\n        num_epochs=EPOCHS_NUM,\n        device=device,\n        model_name='Transformer',\n        num_classes=4\n    )\n    end_time = time.time()\n    hybrid_training_time = end_time - start_time\n\n    csv_path = './logs/Transformer_metrics.csv'\n    save_metrics_to_csv(hybrid_train_metrics, hybrid_val_metrics, csv_path)\n\n    plot_path = './logs/Transformer_plots.png'\n    plot_metrics(hybrid_train_metrics, hybrid_val_metrics, plot_path)\n\n    # Log HybridFPNTransformer training time and parameters\n    time_metrics.append({\n        'Model': 'Transformer',\n        'Training Time (seconds)': hybrid_training_time,\n        'Number of Parameters': sum(p.numel() for p in hybrid_model.parameters() if p.requires_grad)\n    })\n\n    # Clear VRAM\n    del hybrid_model, criterion, optimizer, trained_hybrid_model\n    torch.cuda.empty_cache()\n\n    pd.DataFrame(time_metrics).to_csv(time_metrics_path, index=False)\n\n    hybrid_model_gry = HybridFPNTransformer_gry(num_classes=4).to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(\n        hybrid_model_gry.parameters(),\n        lr=1e-4,\n        weight_decay=1e-5  # L2 regularization\n    )\n\n    start_time = time.time()\n    hybrid_model_gry, hybrid_train_metrics_gry, hybrid_val_metrics_gry = train_model_enhanced(\n        model=hybrid_model_gry,\n        # train_loader=train_dataset_gry,\n        train_loader=train_loader_gry,\n        val_loader=val_loader_gry,\n        criterion=criterion,\n        optimizer=optimizer,\n        num_epochs=EPOCHS_NUM,\n        device=device,\n        model_name='HybridFPNTransformerGRY',\n        num_classes=4\n    )\n    end_time = time.time()\n    hybrid_gry_training_time = end_time - start_time\n\n    csv_path = './logs/HybridFPNTransformerGRY_metrics.csv'\n    save_metrics_to_csv(hybrid_train_metrics_gry, hybrid_val_metrics, csv_path)\n\n    plot_path = './logs/HybridFPNTransformerGRY_plots.png'\n    plot_metrics(hybrid_val_metrics_gry, hybrid_val_metrics, plot_path)\n\n    # Log HybridFPNTransformer training time and parameters\n    time_metrics.append({\n        'Model': 'HybridFPNTransformerGRY',\n        'Training Time (seconds)': hybrid_gry_training_time,\n        'Number of Parameters': sum(p.numel() for p in hybrid_model_gry.parameters() if p.requires_grad)\n    })\n\n    # Clear VRAM\n    # del hybrid_model_gry, criterion, optimizer, hybrid_model_gry\n    del hybrid_model_gry, criterion, optimizer\n    torch.cuda.empty_cache()\n\n    pd.DataFrame(time_metrics).to_csv(time_metrics_path, index=False)\n\nif __name__ == '__main__':\n    main()\n","metadata":{"id":"VXTmnJ7OTEq5","trusted":true,"execution":{"iopub.status.busy":"2025-01-07T14:13:10.156091Z","iopub.execute_input":"2025-01-07T14:13:10.156364Z","iopub.status.idle":"2025-01-07T14:17:39.833531Z","shell.execute_reply.started":"2025-01-07T14:13:10.156332Z","shell.execute_reply":"2025-01-07T14:17:39.832538Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-7-af5be1ee0dfc>:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  base_model.load_state_dict(torch.load(weights_path))\n","output_type":"stream"},{"name":"stdout","text":"0.9169462386221044\nMetrics saved to ./logs/VGG16_metrics.csv\nSaved plot for f1_score to ./logs/VGG16_plots.png/f1_score.png\nSaved plot for precision to ./logs/VGG16_plots.png/precision.png\nSaved plot for recall to ./logs/VGG16_plots.png/recall.png\nSaved plot for sensitivity to ./logs/VGG16_plots.png/sensitivity.png\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-7-af5be1ee0dfc>:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  base_model.load_state_dict(torch.load(weights_path))\n","output_type":"stream"},{"name":"stdout","text":"0.9469187706460407\nMetrics saved to ./logs/ResNet18_metrics.csv\nSaved plot for f1_score to ./logs/ResNet18_plots.png/f1_score.png\nSaved plot for precision to ./logs/ResNet18_plots.png/precision.png\nSaved plot for recall to ./logs/ResNet18_plots.png/recall.png\nSaved plot for sensitivity to ./logs/ResNet18_plots.png/sensitivity.png\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-7-af5be1ee0dfc>:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  base_model.load_state_dict(torch.load(weights_path))\n","output_type":"stream"},{"name":"stdout","text":"0.8874385662214457\nMetrics saved to ./logs/AlexNet_metrics.csv\nSaved plot for f1_score to ./logs/AlexNet_plots.png/f1_score.png\nSaved plot for precision to ./logs/AlexNet_plots.png/precision.png\nSaved plot for recall to ./logs/AlexNet_plots.png/recall.png\nSaved plot for sensitivity to ./logs/AlexNet_plots.png/sensitivity.png\n0.7158969294707761\nMetrics saved to ./logs/Transformer_metrics.csv\nSaved plot for f1_score to ./logs/Transformer_plots.png/f1_score.png\nSaved plot for precision to ./logs/Transformer_plots.png/precision.png\nSaved plot for recall to ./logs/Transformer_plots.png/recall.png\nSaved plot for sensitivity to ./logs/Transformer_plots.png/sensitivity.png\n0.5527006913581265\nMetrics saved to ./logs/HybridFPNTransformerGRY_metrics.csv\nSaved plot for f1_score to ./logs/HybridFPNTransformerGRY_plots.png/f1_score.png\nSaved plot for precision to ./logs/HybridFPNTransformerGRY_plots.png/precision.png\nSaved plot for recall to ./logs/HybridFPNTransformerGRY_plots.png/recall.png\nSaved plot for sensitivity to ./logs/HybridFPNTransformerGRY_plots.png/sensitivity.png\n","output_type":"stream"}],"execution_count":7}]}